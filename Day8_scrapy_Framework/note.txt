scrapy框架

-什么是框架？
    - 就是一个继承了很多功能并且具有很强通用性的一个项目模板
-如何学习框架？
    - 专门学习框架封装额各种功能的详细用法。

-什么是scrapy？
    - 爬虫中封装好的一个明星框架。
    - 功能：
        - 高性能的持久化存储
        - 异步的数据下载
        - 高性能的数据解析
        - 分布式

-scrapy框架的基本使用
    - 环境安装：
        - mac or linux： pip install scrapy
        - Windows：
            - pip install wheel
            - 下载twisted， 下载地址为
                - https://www.lfd.uci.edu/~gohlke/pythonlibs/#Twisted
            - 安装twisted： pip install Twisted-17.1.0-cp36-cp36m-win_amd64.whl
                           pip install pywin32
                           pip install scrapy
            -测试： 在终端里输入scrapy指令，没有报错安装成功。
    - 创建一个工程： scrapy startproject xxxPro
    - cd xxxPro
    - 在spiders目录中创建爬虫文件
        - scrapy genspider spiderName www.xxx.com
    - 执行工程：
        - scrapy crawl spiderName
-scrapy数据分析

-scrapy持久化存储
    - 基于终端指令：
        - 要求：只可以将parse（）方法的返回值存储到本地的文本文件中
        - 注意： 持久化存储对应的文本文件的类型只可以为'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle')
        - 指令： scrapy crawl spiderName -o filePath
        - 好处： 简洁高效
        - 缺点： 局限性比较强（数据只可以存储到指定后缀的文本文件中）
    - 基于管道：
        - 编码流程：
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据封装存储到item类型的对象中
                -author = scrapy.Field()
                -content = scrapy.Field()
            - 将item类型的对象提交到pipelines进行持久化存储的操作
            - 在pipelines中的process_item要将接收到的item对象中存储的数据进行持久化存储
            - 在配置文件中开启pipeline，默认不开启pipeline，需手动开启
        -好处:
            - 通用性强
        -缺点:
            - 编码稍显繁琐
    - 面试题: 将爬取到的数据,一份存本地,一份存数据库.
        - 管道文件中一个管道类对应的是将数据存储到一种平台
        - 爬虫文件提交的item指挥给管道文件中第一个被执行的管道类接受
        - process_item中的return item 表示将item传递给下一个即将被执行的管道类
-基于spider的全站数据爬取
    - 就是将网站中某板块中的全部页码对应的页面进行爬取和解析
    - 需求：爬取校花网中照片的名称
    - 实现方式：
            - 将所有页面全部加到url list中
            - 自行手动进行请求发送（推荐）
                - 手动请求发送：callback 回调函数是专门用作数据解析
                    - yield scrapy.Request(url=new_url, callback=self.parse)
- 五大核心组件
    - Scrapy Engine
        - 引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。
    - 调度器(Scheduler)
        - 调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。
    - 下载器(Downloader)
        - 下载器负责获取页面数据并提供给引擎，而后提供给spider。
    - Spiders
        - Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 更多内容请看 Spiders 。
    - Item Pipeline
        - Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。 更多内容查看 Item Pipeline 。

- 请求传参
    - 使用场景： 如果要爬取解析的数据不在同一页面，需要深度爬取。
    - 需求： 爬取boss直聘岗位职称和描述





