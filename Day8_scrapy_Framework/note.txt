scrapy框架

-什么是框架？
    - 就是一个继承了很多功能并且具有很强通用性的一个项目模板
-如何学习框架？
    - 专门学习框架封装额各种功能的详细用法。

-什么是scrapy？
    - 爬虫中封装好的一个明星框架。
    - 功能：
        - 高性能的持久化存储
        - 异步的数据下载
        - 高性能的数据解析
        - 分布式

-scrapy框架的基本使用
    - 环境安装：
        - mac or linux： pip install scrapy
        - Windows：
            - pip install wheel
            - 下载twisted， 下载地址为
                - https://www.lfd.uci.edu/~gohlke/pythonlibs/#Twisted
            - 安装twisted： pip install Twisted-17.1.0-cp36-cp36m-win_amd64.whl
                           pip install pywin32
                           pip install scrapy
            -测试： 在终端里输入scrapy指令，没有报错安装成功。
    - 创建一个工程： scrapy startproject xxxPro
    - cd xxxPro
    - 在spiders目录中创建爬虫文件
        - scrapy genspider spiderName www.xxx.com
    - 执行工程：
        - scrapy crawl spiderName
-scrapy数据分析

-scrapy持久化存储
    - 基于终端指令：
        - 要求：只可以将parse（）方法的返回值存储到本地的文本文件中
        - 注意： 持久化存储对应的文本文件的类型只可以为'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle')
        - 指令： scrapy crawl spiderName -o filePath
        - 好处： 简洁高效
        - 缺点： 局限性比较强（数据只可以存储到指定后缀的文本文件中）
    - 基于管道：
        - 编码流程：
            - 数据解析
            - 在item类中定义相关的属性
            - 将解析的数据封装存储到item类型的对象中
                -author = scrapy.Field()
                -content = scrapy.Field()
            - 将item类型的对象提交到pipelines进行持久化存储的操作
            - 在pipelines中的process_item要将接收到的item对象中存储的数据进行持久化存储
            - 在配置文件中开启pipeline，默认不开启pipeline，需手动开启
        -好处:
            - 通用性强
        -缺点:
            - 编码稍显繁琐
    - 面试题: 将爬取到的数据,一份存本地,一份存数据库.
        - 管道文件中一个管道类对应的是将数据存储到一种平台
        - 爬虫文件提交的item指挥给管道文件中第一个被执行的管道类接受
        - process_item中的return item 表示将item传递给下一个即将被执行的管道类
-基于spider的全站数据爬取
    - 就是将网站中某板块中的全部页码对应的页面进行爬取和解析
    - 需求：爬取校花网中照片的名称
    - 实现方式：
            - 将所有页面全部加到url list中
            - 自行手动进行请求发送（推荐）
                - 手动请求发送：callback 回调函数是专门用作数据解析
                    - yield scrapy.Request(url=new_url, callback=self.parse)
- 五大核心组件
    - Scrapy Engine
        - 引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。
    - 调度器(Scheduler)
        - 调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。
    - 下载器(Downloader)
        - 下载器负责获取页面数据并提供给引擎，而后提供给spider。
    - Spiders
        - Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 更多内容请看 Spiders 。
    - Item Pipeline
        - Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。 更多内容查看 Item Pipeline 。

- 请求传参
    - 使用场景： 如果要爬取解析的数据不在同一页面，需要深度爬取。
    - 需求： 爬取boss直聘岗位职称和描述
- 图片数据爬取： ImagePipeLine
    - 基于scrapy爬取字符串类型的数据和图片的数据有什么区别？
        - 字符串： 只需要基于xpath进行解析且提交管道进行持久化存储即可
        - 图片： xpath解析出的图片你src地址的属性值，单独对图片地址发起请求获取图片二进制类型的数据。

    - ImagePipeLine：
        - 只需要将img的src属性值进行解析，提交管道，管道会对图片的src进行请求发送获取二进制类型的数据，然后进行持久化存储。
    - 使用流程：
        - 需求： 爬取站长素材中的图片
        - 使用流程：
            - 数据解析（图片地址）
            - 将存储图片地址的item提交到指定的管道类
            - 在管道文件中自定义一个基于ImagesPipeLine的一个管道类
                - get_media_requests
                - file_path
                - item_completed
            - 在setting中：
                - 指定图片存储的目录：IMAGES_STORE = './imgs'
                - 指定开启的管道：到自定义的管道类
- 中间件：
    - 下载中间件：引擎和下载器之间的控件
        - 作用： 批量拦截到整个工程中所有的请求和响应
        - 拦截请求：
            - UA伪装：setting的UA伪装是对全局的伪装，而中间件中的伪装可以用于不同的对象
                -  process_request
            - 代理IP： 同伪装，为每个请求设置不同的IP
                - process_exception -> return request
        - 拦截响应：
            - 篡改响应数据，响应对象
            - 需求：爬取网易新闻中的新闻数据（标题和内容）
                - 1. 通过网易新闻的首页解析出五大板块对应的详情页url（无动态加载）
                - 2. 每一个板块对应的新闻标题都是动态加载出来的（动态加载）
                - 3. 通过解析出的每一条新闻详情页的url获取详情页的页面源码，解析出新闻内容

    - 爬虫中间件：引擎和spider之间的控件

- CrawSpider：基于Spider的一个子类
    - 专门全站数据爬取的方式
        - 基于Spider：手动请求发送
        - 基于CrawlSpider
    - CrawlSpider的使用：
        - 创建工程：
        - cd xxx
        - 创建爬虫文件（CrawlSpider）：
            - scrapy genspider -t crawl xxx www.xx.com
            - 链接提取器：
                - 作用： 从start_url中根据指定规则进行指定文件的提取
            - 规则提取器：
                - 作用：将链接提取器提取到的链接进行指定规则（callback）的解析操作
        - 需求：爬取sun网站的编号，新闻标题，新闻内容，标号
            - 分析： 爬取的数据没有在同一张页面中
            - 步骤：
                - 1. 用链接提取器提取所有页面链接
                - 2. 让链接提取器提取所有新闻详情页的链接
- 分布式爬虫
    -概念：需要搭建一个分布式的集群（多台电脑），让其对一组资源进行分布联合爬取。
    -作用：提升爬取数据的效率

    -如何时间分布式？
        - 安装scrapy-redis
        - 原生的scrapy是不能实现分布式的， 必须用redis组件实现分布式爬虫
        -